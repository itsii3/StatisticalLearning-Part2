---
title: "Decision Trees"
author: "Ivet A., Laura A., Arnau G."
output:
  pdf_document:
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE,
                      fig.height = 4, fig.width = 6,
                      fig.align = "center")
```

# Introduction

File `HFCRD.csv` contains the *Heart Failure Clinical Records Dataset* with information on 299 patients with advanced heart failure in whom the following variables were analyzed:

- `age`: age of the patient (years)
- `anaemia`: decrease of red blood cells or - hemoglobin (boolean)
- `creatinine_phosphokinase`: level of the CPK enzyme in the blood $(\mathrm{mcg} / \mathrm{L})$
- `diabetes`: if the patient has diabetes (boolean)
- `ejection_fraction`: ejection fraction: percentage of blood leaving the heart at each contraction
- `high_blood_pressure`: if the patient has hypertension (boolean)
- `platelets`: platelets in the blood (kiloplatelets/mL)
- `serum_creatinine`: level of serum creatinine in the blood $(\mathrm{mg} / \mathrm{dL})$
- `serum_sodium`: level of serum sodium in the blood $(\mathrm{mEq} / \mathrm{L})$
- `sex`: female/male (binary)
- `smoking`: if the patient smokes or not (boolean)
- `time`: follow-up period (days)
- `DEATH_EVENT`: the patient deceased during the follow-up period (boolean)

Let's load the data:

```{r}
data <- read.csv("HFCRD.csv")
```

# Exploratory data analysis

First of all, notice that many variables are described as *boolean*, but they are stored as *numerical* in the data frame. So let's save as factors variables which should be treated as that:

```{r}
# Features
hfcrd <- data
hfcrd$anaemia <- as.factor(hfcrd$anaemia)
hfcrd$diabetes <- as.factor(hfcrd$diabetes)
hfcrd$high_blood_pressure <- as.factor(hfcrd$high_blood_pressure)
hfcrd$sex <- as.factor(hfcrd$sex)
hfcrd$smoking <- as.factor(hfcrd$smoking)
# Target
hfcrd$DEATH_EVENT <- as.factor(hfcrd$DEATH_EVENT)
```

Let's see how is the distribution of the target:

```{r}
survival_table <- table(hfcrd$DEATH_EVENT)
barplot(survival_table, 
        main = "# Observations by response", 
        xlab = "Survival", 
        col = "turquoise")
```

It seems we have available much more data (twice) about individuals who ended up surviving after the study than from the ones who ended up dying. This could difficult the prediction for the latter cases.

The next chunk displays a quite complete description of the features:

```{r}
library(skimr)
skim(hfcrd[, -13])
```

We can wee the absence of missing values in the data, which is nice. The other factor variables seem also to be much unbalanced, but we hope that this fact won't affect negatively the training process.

Numerical variables, in general, do not show a symmetric distribution neither.

Another analysis that's often considered when working with data sets is a PCA. In this case we do not have a large amount of variables, so it is not a quite *mandatory* technique. However, it could be interesting to check its results.

Notice that we use the original data set `data` because the `prcomp()` needs the data to be of type numeric (instead of factor)

```{r}
hfcrd_PCA <- prcomp(data, scale = TRUE)
```

The following function will plot the variance explained by each component:

```{r}
library(factoextra)
fviz_screeplot(hfcrd_PCA, addlabels = TRUE, ylim = c(0, 20))
```

By looking at the previous plot we can notice how important and necessary are all the variables for our analysis, since all of them are needed in order to explain the variability of the data.

With 2 components, less than a 30\% of the data is explained... So we won't discard any feature.

One last interesting plot is the following, which shows the distribution of the first and second components by each target level:

```{r}
fviz_pca_ind(hfcrd_PCA, geom.ind = "point", 
             col.ind = hfcrd$DEATH_EVENT, 
             axes = c(1, 2), 
             pointsize = 1.5)
```

The first dimension seems responsible of capturing the trend of the target variable. This fact sounds good to us because it could mean that `DEATH_EVENT` is an important value related to the rest of variables.

# Data preparation

Before moving into the fitting of any model, data should be prepared. As scaling is not necessary when working with trees, the only step we are going to consider is spiting the data into 2 sets: a training set $(2 / 3)$ and a test set $(1 / 3)$.

```{r}
set.seed(1234)
n <- nrow(features)
train_prop <- 2/3
train_ind <- sample(1:n, train_prop*n)
hfcrd_test <- hfcrd[-train_ind, ]
survival_test <- hfcrd[-train_ind, "DEATH_EVENT"]
```

# Model fitting

In this section we are going to fit the following statistical models:

1. A classification tree to predict survival.
2. A logistic classifiers with the same goals and assessment.
3. A third predictive model of our choice: ARNAU AQUÍ EL NOM DEL QUE TRIES

For each one of them we'll do the required tuning, if needed, train the model and do a proper test-based evaluation.

## Classification tree

**IVET**

```{r}
library(tree)
hfcrd_tree <- tree(DEATH_EVENT ~ ., hfcrd,
                   subset = train_ind,
                   split = "deviance")
```

```{r}
summary(hfcrd_tree)
```

```{r}
plot(hfcrd_tree)
text(hfcrd_tree,pretty=0, cex=0.6)
```

```{r}
tree.pred=predict(hfcrd_tree,hfcrd_test,type="class")
res <- table(tree.pred,survival_test)
res
accrcy <- sum(diag(res)/sum(res))
```

## Logistic classifier

**LAURA**

## Bayesian???

**ARNAU**

# Conclusions

*Compare the predictors based on the adequate metrics.*




