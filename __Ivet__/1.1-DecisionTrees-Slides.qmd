---
title: "Tree based methods"
author: "Ivet Acosta"
format:
    html: 
      toc: true
      toc-float: true
      number-sections: true
      df-print: paged
      toc-depth: 3
      fig-width: 6
      fig-height: 4
      fontsize: "14px"
      max-width: "100%"
      margin-left: "0pt"
      margin-right: "0pt"
      embed-resources: true
knit:
  quarto:
    chunk_options:
      echo: true
      cache: false
      prompt: false
      tidy: true
      comment: NA
      message: false
      warning: false
    knit_options:
      width: 75
reference-location: margin
execute:
    echo: true
    message: false
    warning: false
editor_options: 
  chunk_output_type: console
---

# Introduction to Decision Trees

## Motivation

-   In many real-world applications, decisions need to be made based on complex, multi-dimensional data.
-   One goal of statistical analysis is to provide insights and guidance to support these decisions.
-   Decision trees provide a way to organize and summarize information in a way that is easy to understand and use in decision-making.

**Examples:**

-   A bank needs to have a way to decide if/when a customer can be granted a loan.
-   A doctor may need to decide if a patient has to undergo a surgery or a less aggressive treatment.
-   A company may need to decide about investing in new technologies or stay with the traditional ones.

In all those cases a decision tree may provide a structured approach to decision-making that is based on data and can be easily explained and justified.

**An intuitive approach.**
Decisions are often based on asking several questions on available information whose answers induce binary splits on data that end up with some grouping or classification.

```{r fig.align='center', out.width="50%", fig.cap='A doctor may classify patients at high or low cardiovascular risk using some type of decision tree'}
#| code-fold: true
knitr::include_graphics("Assets/decTree4HypertensionNew.png")
```

## *What is a decision tree?*

-   A decision tree is a graphical representation of a series of decisions and their potential outcomes.
-   It is obtained by recursively *stratifying* or *segmenting* the *feature space* into a number of simple regions.
-   Each region (decision) corresponds to a *node* in the tree, and each potential outcome to a *branch*.
-   The tree structure can be used to guide decision-making based on data.

_**What do we need to learn?**_

- We need **context**: 

    - When is it appropriate to rely on decision trees?
    - When would other approaches be preferable?
    - What type of decision trees can be used?

- We need to know how to **build good trees**:

    - How do we *construct* a tree?
    - How do we *optimize* the tree?
    - How do we *evaluate* it?

### More context

Decision trees are non parametric, data guided predictors, well suited in many situations such as:

- Non-linear relationships.
- High-dimensional data.
- Interaction between variables exist.
- Mixed data types.

They are not so appropriate for complex datasets, or complex problems, that require expert knowledge.

[See here some examples of each situation.](https://g.co/gemini/share/781b7d88e03a)

### Types of decision trees

- **Classification Trees** are built when the response variable is categorical.
    - They aim to *classify a new observation* based on the values of the predictor variables.

- **Regression Trees** are used when the response variable is numerical.
    - They aim to *predict the value* of a continuous response variable based on the values of the predictor variables.

## Tree building

**Tree bulding with `R`:**

| **Package** | **Algorithm** | **Dataset size** | **Missing data handling** | **Ensemble methods** | **Visual repr** | **User interface** |
|-------------|---------------|------------------|---------------------------|----------------------|----------------------------|--------------------|
| [**`rpart`**](https://cran.r-project.org/web/packages/rpart/index.html) | RPART         | Medium to large  | Poor                      | No                   | Yes                        | Simple             |
| [**`caret`**](https://topepo.github.io/caret/) | Various       | Various          | Depends on algorithm      | Yes                  | Depends on algorithm       | Complex            |
| [**`tree`**](https://cran.r-project.org/web/packages/tree/index.html)  | CART          | Small to medium  | Poor                      | No                   | Yes                        | Simple             |


<br>
**Tree building with `Python`:**
<br>

| **Package**        | **Algorithm**        | **Dataset size** | **Missing data handling** | **Ensemble methods** | **Visual repr** | **User interface** |
|-------------------|----------------------|------------------|---------------------------|----------------------|----------------------------|--------------------|
| **`scikit-learn`** | CART (DecisionTreeClassifier) | Small to large | Can handle NaN | Yes | Yes (using Graphviz) | Simple |
| **`dtreeviz`**     | CART (DecisionTree) | Small to large | Can handle NaN | No | Yes | Simple |
| **`xgboost`**      | Gradient Boosting    | Medium to large  | Requires imputation       | Yes                  | No                         | Complex            |
| **`lightgbm`**     | Gradient Boosting    | Medium to large  | Requires imputation       | Yes                  | No                         | Complex            |

## Starting with an example

The Pima Indian Diabetes dataset contains 768 individuals (female) and 9 clinical variables.

```{r}
data("PimaIndiansDiabetes2", package = "mlbench")
dplyr::glimpse(PimaIndiansDiabetes2)
```

These Variables are known to be related with cardiovascular diseases, so it seems intuitive to use these variables to decide if a person is affected by diabetes


```{r}
library(magrittr)
descAll <- as.data.frame(skimr::skim(PimaIndiansDiabetes2))
desc <- descAll[,c(10:15)]
rownames(desc) <- descAll$skim_variable
colnames(desc) <- colnames(desc) %>% stringr::str_replace("numeric.", "")
desc
```

### Predicting Diabetes onset

We wish to predict the probability of individuals in being diabete-positive or negative.

We start building a tree with all the variables:

```{r echo=TRUE}
library(rpart)
model1 <- rpart(diabetes ~., data = PimaIndiansDiabetes2)
```

---

<details>
<summary><b>Viewing the tree as text</b></summary><br>

```{r}
model1
```

</details>

---

- This representation shows the variables and split values that have been selected by the algorithm.
- It can be used to classify (new) individuals following the decisions (splits) from top to bottom.

### Plotting the tree

A simple visualization illustrates how it proceeds  

```{r fig.cap="Even without domain expertise the model seems *reasonable*",out.height="10cm"}
plot(model1)
text(model1, digits = 3, cex=0.7)
```

It can be plotted nicer:

```{r, fig.cap = "The tree plotted with the `rpart.plot` package."}
require(rpart.plot)
rpart.plot(model1, cex=.7)
```

Each node shows: 

1. the predicted class ('neg' or 'pos'), 
2. the predicted probability,
3. the percentage of observations in the node. 

### Individual prediction

Consider individuals 521 and 562

```{r}
(aSample <- PimaIndiansDiabetes2[c(521, 562),])
```

```{r}
predict(model1, aSample, "class")
```

- If we follow individuals 521 and 562 along the tree, we reach the same prediction.
- The tree provides not only a classification but also an explanation.


### How accurate is the model?

It is straightforward to obtain a simple performance measure:

```{r echo=TRUE}
predicted.classes <- predict(model1, PimaIndiansDiabetes2, "class")
mean(predicted.classes == PimaIndiansDiabetes2$diabetes)
```

The question becomes harder when we go back and ask if *we obtained the best possible tree*.

In order to answer this question we must study tree construction in more detail.

# Building Classification Trees

## First, remember...

As with any model, we aim not only at construting trees. We wish to build good trees and, if possible, optimal trees in some sense we decide.

In order to **build good trees** we must decide:

- How to *construct* a tree?
- How to *optimize* the tree?
- How to *evaluate* it?

**TREES ARE SUPERVISED LEARNERS.**
Classification and Regression are *Supervised Learning* tasks:

- There is a *learning set* $\mathcal{L}=\{(\mathbf{X_i,Y_i})\}_{i=1}^n$
- And depending of $\mathbf{Y}$ we have:
  - Classification: $\mathbf{X}\in\mathbb{R}^d,\quad Y\in\{-1,+1\}
$
  - Regression
$\mathbf{X}\in\mathbb{R}^d,\quad Y\in\mathbb{R}$.

**General Trees** VS **Decision Trees**:

- A **tree** is a set of nodes and edges organized in a hierarchical fashion. In contrast to a graph, in a tree there are no loops.
- A **decision tree** is a tree where *each split node stores a boolean test function* to be applied to the incoming data. Each leaf stores the final answer (predictor).

**Notation.**

-  A node is denoted by $t$. 
    - The left and right child nodes are denoted by $t_{L}$ and  $t_{R}$ respectively.
-   The collection of all nodes in the tree is denoted $T$ 
-   The collection of all the leaf nodes is denoted $\tilde{T}$
-   A split will be denoted by $s$. 
    - The set of all splits is denoted by $S$.

## Building a tree

A binary decision tree is built by defining a series of (recursive) splits on the feature space. The splits are decided in such a way that the associated learning task is attained by setting thresholds on the variables values that induce paths in the tree.

The ultimate goal of the tree is to be able to use a combination of the splits to accomplish the learning task with as small an error as possible.
  
### Trees partition the space

**A tree represents a recursive splitting of the space.**

-  Every node of interest corresponds to one region in the original space.
-   Two child nodes occupy two different regions.
-   Together, yield same region as that of the parent node.

In the end, every leaf node is assigned with a class and a test point is assigned with the class of the leaf node it lands in.

---

<details>
<summary><b>The tree represents the splitting:</b></summary><br>

```{r , fig.align ='center',  out.width="60%"}
#| code-fold: true
knitr::include_graphics("Assets/splits_nodes_1.png")
```

```{r  , fig.align ='center', out.width="60%"}
#| code-fold: true
knitr::include_graphics("Assets/splits_nodes_2.png")
```

```{r  , fig.align ='center', out.width="60%"}
#| code-fold: true
knitr::include_graphics("Assets/splits_nodes_3.png")
```

</details>

---

### Different splits are  possible

It is always possible to split a space in distinct ways:

```{r , fig.align ='center', out.width="80%"}
#| code-fold: true
knitr::include_graphics("Assets/1.1-DecisionTrees-Slides_insertimage_3.png")
```

Some ways perform better than other for a given task, but rarely will they be perfect. So we aim at combining splits to find a better rule.

## Construction of a tree

Tree building involves the following three  elements:

1.  The selection of the splits, i.e., *how do we decide which node (region) to split and how to split it?*
    -   *How to select from the pool of candidate splits?*
    -   *What are appropriate __goodness of split__ criteria?*
2.  If we know how to make splits ('grow' the tree), *how do we decide when to declare a node terminal and __stop splitting__?*
3. *How do we assign each terminal node to a class?*

### Split selection

To build a Tree, questions have to be generated that induce splits based on the value of a single variable.

- **Ordered** variable $X_j$:
  - *Is  $X_j \leq c$?* for all possible thresholds $c$.
  - Split lines: parallel to the coordinates.

- **Categorical** variables, $X_j \in \{1, 2, \ldots, M\}$:
  - *Is $X_j \in A$?*, where $A \subseteq M$ .

The pool of candidate splits for all $p$ variables is formed by combining all the generated questions.

### Goodness of Split

The way we choose the split, is *to measure every split by a 'goodness of split' measure*, which depends on:

- the split question
- the node to split.

**Goodness** of split is measured by **impurity functions**.
Intuitively, when we split the points we want the region corresponding to each leaf node to be "pure", that is, most points in this region come from the same class, that is, one class dominates.

---

<details>
<summary><b>Good splits vs bad splits:</b></summary><br>

- *Purity* not increased:

```{r out.width="50%"}
#| code-fold: true
knitr::include_graphics("Assets/BadSplit.png")
```

- *Purity* increased:

```{r out.width="50%"}
#| code-fold: true
knitr::include_graphics("Assets/GoodSplit.png")
```

</details>

---

### Impurity functions

In order to measure homogeneity, or as called here, *purity*, of splits we introduce 

- Impurity functions
- Impurity measures

Used to measure the extent of *purity* for a region containing data points from possibly different classes.

An **impurity function** is a function $\Phi$ defined on the set of all $K$-tuples of numbers $\mathbf{p}= \left(p_{1}, \cdots, p_{K}\right)$ s.t. $p_{j} \geq 0, \,  \sum_{j=1}^K p_{j}=1$,
$$
\Phi: \left(p_{1}, \cdots, p_{K}\right) \rightarrow [0,1]
$$
with the properties:

1.  $\Phi$ achieves maximum only for the uniform distribution, that is all the $p_{j}$ are equal.
2.  $\Phi$ achieves minimum only at the points $(1,0, \ldots, 0)$,$(0,1,0, \ldots, 0)$, $\ldots,(0,0, \ldots, 0,1)$, i.e., when the probability of being in a certain class is 1 and 0 for all the other classes.
3.  $\Phi$ is a symmetric function of $p_{1}, \cdots, p_{K}$, i.e., if we permute $p_{j}$, $\Phi$ remains constant.

The functions below are commonly used to measure impurity.

- $\Phi_E (\mathbf{p}) = -\sum_{j=1}^K p_j\log (p_j)$ (**Entropy**).
- $\Phi_G (\mathbf{p}) = 1-\sum_{j=1}^K p_j^2$. (**Gini Index**).
- $\Phi_M (\mathbf{p}) = \sum_{i=1}^K p_j(1-p_j)$ (**Misclassification rate**).

In practice, for classification trees only the first two are recommended.

*Node impurity functions for the two-class case.* The entropy function (rescaled) is the red curve, the Gini index is the green curve, and the resubstitution estimate of the misclassification rate is the blue curve:

<center>
![](Assets/impurity.jpg){width=65%}
</center>

### Impurity and Goodness of a split

Given an impurity function $\Phi$,  a node $t$,  and given $p(j \mid t)$, the estimated posterior probability of class $j$ given node $t$, the __*impurity measure of $t$*__, $i(t)$, is defined as:
$$
i(t)=\phi(p(1 \mid t), p(2 \mid t), \ldots, p(K \mid t))
$$
That is, the *impurity measure* of a split (or a node) is the impurity function when computed on probabilities associated (conditional) with a node.

Once we have defined $i(t)$, we define the __*goodness of split $s$*__ for node $t$, denoted by $\Phi(s, t)$:
$$
\Phi(s, t)=\Delta i(s, t)=i(t)-p_{R} i\left(t_{R}\right)-p_{L} i\left(t_{L}\right)
$$

The best split for the single variable $X_{j}$ is the one that has the largest value of $\Phi(s, t)$ over all $s \in \mathcal{S}_{j}$, the set of possible distinct splits for $X_{j}$.

### Impurity score of a node

The impurity, $i(t)$, of a node is based solely on the estimated posterior probabilities of the classes. That is, *it doesn't account for the size of $t$*.
  
This is done by the __*impurity score*__ of  $t$, defined as $I(t)=i(t)\cdot p(t)$, a *weighted impurity measure* of node $t$ that takes into account:

- The estimated posterior probabilities of the classes, 
- The estimated proportion of data that go to node $t$.

$I(t)$ can be used to:

- Define the aggregated impurity of a tree, by adding the impurity scores of all terminal leaves.
- Provide a weighted measure of impurity decrease for a split: $\Delta I(s, t)=p(t) \Delta i(s, t)$.
- Define a criteria for stop splitting a tree (see below).

### Entropy as an impurity measure

The __*entropy of a node*__, $t$, that is split in $n$ child nodes $t_1$, $t_2$, ..., $t_n$, is: 
$$
H(t)=-\sum_{i=1}^{n} P\left(t_{i}\right) \log _{2} P\left(t_{i}\right)
$$

From here, an information gain (that is impurity decrease) measure can be introduced. That is, an information theoretic approach that compares

- the entropy of the parent node before the split to 
- that of a weighted sum of the child nodes after the split where the weights are proportional to the number of observations in each node. 

For a split $s$ and a set of observations (a node) $t$, __*information gain*__ is defined as:
$$
\begin{aligned}
& IG(t, s)=\text { (original entr.) }-(\text { entr. after split) } \\
& IG(t, s)=H(t)-\sum_{i=1}^{n} \frac{\left|t_{i}\right|}{t} H\left(x_{i}\right)
\end{aligned}
$$

### Example

Consider the problem of designing an algorithm to automatically differentiate between apples and pears (class labels) given only their width and height measurements (features).

<details>
<summary><b>Data:</b></summary><br>

| **Width** | **Height** | **Fruit** |
|-----------|------------|-----------|
| 7.1       | 7.3        | Apple     |
| 7.9       | 7.5        | Apple     |
| 7.4       | 7.0        | Apple     |
| 8.2       | 7.3        | Apple     |
| 7.6       | 6.9        | Apple     |
| 7.8       | 8.0        | Apple     |
| 7.0       | 7.5        | Pear      |
| 7.1       | 7.9        | Pear      |
| 6.8       | 8.0        | Pear      |
| 6.6       | 7.7        | Pear      |
| 7.3       | 8.2        | Pear      |
| 7.2       | 7.9        | Pear      |

</details>


- **Entropy Calculation:**

```{r out.width="100%"}
#| code-fold: true
knitr::include_graphics("Assets/Example2-EntropyCalculation.png")
```

- **Information Gain:**

```{r out.width="100%"}
#| code-fold: true
knitr::include_graphics("Assets/Example2-IGCalculation.png")
```

# Prediction with Trees

## Class Assignment

The decision tree classifies new data points as follows:

- We let a data point pass down the tree and see which leaf node it lands in.
- The class of the leaf node is assigned to the new data point. Basically, all the points that land in the same leaf node will be given the same class. 
- This is similar to k-means or any prototype method.

A **class assignment rule** assigns a class $j=1, \ldots, K$ to every terminal (leaf) node $t \in \tilde{T}$.
The class is assigned to node $t$ is denoted by $\kappa(t)$. *E.g.,* if $\kappa(t)=2$, all the points in node $t$ would be assigned to class 2.
  
If we use 0-1 loss, the class assignment rule picks the class with maximum posterior probability:
$$
\kappa(t)=\arg \max _{j} p(j \mid t)
$$

## Estimating the error rate

Let's assume we have built a tree and have the classes assigned for the leaf nodes. Our **goal** is to estimate *the classification error rate* for this tree.

We use the *resubstitution estimate $r(t)$ for the probability of misclassification, given that a case falls into node $t$*. This is:
$$
r(t)=1-\max _{j} p(j \mid t)=1-p(\kappa(t) \mid t)
$$

Denote $R(t)=r(t) p(t)$, that is the miscclassification error rate weighted by the probability of the node.
The resubstitution estimation for the overall misclassification rate $R(T)$ of the tree classifier $T$ is:
$$
R(T)=\sum_{t \in \tilde{T}} R(t)
$$

# Obtaining best trees

## *When to stop splitting?*

**Maximizing information gain** is one possible criteria to choose among splits.
In order to avoid excessive complexity it is usually decided to stop splitting when *information gain does not compensate for increase in complexity*.

In practice, stop splitting is decided when: 
$$
\max _{s \in S} \Delta I(s, t)<\beta,
$$
where: 

- $\Delta I$ represents the information gain associated with an optimal split $s$ and a node $t$, 
- $\beta$ is a pre-determined threshold.

## Optimizing the Tree

Trees obtained by looking for optimal splits tend to overfit: good for the data in the tree, but generalize badly and tend to fail more in predictions.

In order to reduce complexity and overfitting, while keeping the tree as good as possible, tree __*pruning*__ may be applied.
Pruning works *removing branches that are unlikely to improve the accuracy* of the model on new data.

There are different pruning methods, but the most common one is the __*cost-complexity*__ pruning algorithm, also known as the *weakest link pruning*.
The algorithm works by adding a penalty term to the misclassification rate of the terminal nodes:
$$
R_\alpha(T) =R(T)+\alpha|T|
$$
where $\alpha$ is the parameter that controls the trade-off between tree complexity and accuracy.

### Cost complexity pruning {-}

1. Start by building a large tree that overfits the data.
2. Then, use cross-validation to estimate the optimal value of alpha that minimizes the generalization error.
3. Finally, prune the tree by removing the branches that have a smaller improvement in impurity than the penalty term multiplied by alpha.

Iterate the process until no more branches can be pruned, or until a minimum tree size is reached.

# Regression Trees

## Regression modelling with trees

When the response variable is numeric, decision trees are __*regression trees*__.

They are an option of choice for distinct reasons

- The relation between response and potential explanatory variables is not linear.
- Perform automatic variable selection.
- Easy to interpret, visualize, explain.
- Robust to outliers and can handle missing data

## Classification vs Regression Trees 

| **Aspect**            | **Regression Trees**                                  | **Classification Trees**                            |
|:-----------------|:--------------------------|:--------------------------|
| Outcome var. type     | Continuous                                            | Categorical                                         |
| Goal                  | To predict a numerical value                          | To predict a class label                            |
| Splitting criteria    | Mean Squared Error, Mean Abs. Error                    | Gini Impurity, Entropy, etc.                        |
| Leaf node prediction  | Mean or median of the target variable in that region  | Mode or majority class of the target variable \...  |
| Examples of use cases | Predicting housing prices, predicting stock prices    | Predicting customer churn, predicting high/low risk in diease  |
| Evaluation metric     | Mean Squared Error, Mean Absolute Error, R-square | Accuracy, Precision, Recall, F1-score, etc.         |

## Building the tree

### Splitting

Consider:

- all predictors $X_1, \dots, X_n$, and 
- all values of cutpoint $s$ for each predictor and 

For each predictor find boxes $R_1, \ldots, R_J$ that minimize the RSS, given by:
$$
\sum_{j=1}^J \sum_{i \in R_j}\left(y_i-\hat{y}_{R_j}\right)^2,
$$
where $\hat{y}_{R_j}$ is the mean response for the training observations within the $j$ th box.

To do this, define the pair of half-planes
$$
R_1(j, s)=\left\{X \mid X_j<s\right\} \text { and } R_2(j, s)=\left\{X \mid X_j \geq s\right\}
$$
and seek the value of $j$ and $s$ that minimize the equation:
$$
\sum_{i: x_i \in R_1(j, s)}\left(y_i-\hat{y}_{R_1}\right)^2+\sum_{i: x_i \in R_2(j, s)}\left(y_i-\hat{y}_{R_2}\right)^2.
$$

### Prediction

Once the regions have been created we predict the response using the mean of the trainig observations *in the region to which that observation belongs*.

In the following example, for an observation belonging to the shaded region, the prediction would be:
$$
\hat{y} =\frac{1}{4}(y_2+y_3+y_5+y_9)
$$

```{r fig.align='center', out.width="50%"}
#| code-fold: true
knitr::include_graphics("Assets/RegressionTree-Prediction1.png")
```

## Error estimation and optimization

### Prunning the tree

As before, *cost-complexity prunning* can be applied. We consider a sequence of trees indexed by a nonnegative tuning parameter $\alpha$. For each value of $\alpha$ there corresponds a subtree $T \subset T_0$ such that:
$$
\sum_{m=1}^{|T|} \sum_{y_i \in R_m}
\left(y_i -\hat{y}_{R_m}\right)^2+
\alpha|T|\quad (*)
\label{prunning}
$$
is as small as possible.

### Tuning parameter $\alpha$

$\alpha$ controls a trade-off between the subtree’s complexity and its fit to the training data. 

- When $\alpha=0$, then the subtree $T$
will simply equal $T_0$.
- As $\alpha$ increases, there is a price to pay for having a tree with many terminal nodes, and so (*) will tend to be minimized for a smaller subtree. 

*Note.* Equation (*1)  is reminiscent of the lasso.

**$\alpha$ can be chosen by cross-validation.**

1. Use recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations.
2. Apply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of $\alpha$.
3. Use $K$-fold cross-validation to choose $\alpha$. That is, divide the training observations into $K$ folds. For each $k=1, \ldots, K$:
    - Repeat Steps 1 and 2 on all but the $k$ th fold of the training data.
    - Evaluate the mean squared prediction error on the data in the left-out $k$ th fold, as a function of $\alpha$.
4. Average the results for each value of $\alpha$. Pick $\alpha$ to minimize the average error.
5. Return the subtree from Step 2 that corresponds to the chosen value of $\alpha$.

## Example

The `airquality` dataset from the `datasets` package contains daily air quality measurements
in New York from May through September of 1973 (153 days).
The main variables include:

- Ozone: the mean ozone (in parts per billion) ...
- Solar.R: the solar radiation (in Langleys) ...
- Wind: the average wind speed (in mph) ...
- Temp: the maximum daily temperature (ºF) ...
  
**Main goal**: Predict ozone concentration.

Notice the **non-linear relationships**:

```{r eval=FALSE, echo=TRUE}
aq <- datasets::airquality
color <- adjustcolor("forestgreen", alpha.f = 0.5)
ps <- function(x, y, ...) {  # custom panel function
  panel.smooth(x, y, col = color, col.smooth = "black", cex = 0.7, lwd = 2)
}
pairs(aq, cex = 0.7, upper.panel = ps, col = color)
```

```{r echo=FALSE, fig.align='center'}
aq <- datasets::airquality
color <- adjustcolor("forestgreen", alpha.f = 0.5)
ps <- function(x, y, ...) {  # custom panel function
  panel.smooth(x, y, col = color, col.smooth = "black", 
               cex = 0.7, lwd = 2)
}
pairs(aq, cex = 0.7, upper.panel = ps, col = color)

```

### A regression tree  

```{r echo=TRUE}
set.seed(123)
train <- sample(1:nrow(aq), size = nrow(aq)*0.7)
aq_train <- aq[train,]
aq_test  <- aq[-train,]
aq_regresion <- tree::tree(formula = Ozone ~ ., 
                           data = aq_train, split = "deviance")
summary(aq_regresion)
```

We can plot the tree:

```{r}
par(mar = c(1,1,1,1))
plot(x = aq_regresion, type = "proportional")
text(x = aq_regresion, splits = TRUE, pretty = 0, cex = 0.8, col = "firebrick")
```

### Prune the tree

```{r echo=TRUE, warning=TRUE}
cv_aq <- tree::cv.tree(aq_regresion, K = 5)
optimal_size <-  rev(cv_aq$size)[which.min(rev(cv_aq$dev))]
aq_final_tree <- tree::prune.tree(
                 tree = aq_regresion,
                 best = optimal_size
               )
summary(aq_final_tree)
```

In this example pruning does not improve the tree.

# Advantages and disadvantages of trees

**Trees have many advantages...**

- Trees are very easy to explain to people. 
- Decision trees may be seen as good mirrors of human decision-making.
- Trees can be displayed graphically, and are easily interpreted even by a non-expert.
- Trees can easily handle qualitative predictors without the need to create dummy variables.

**But they come at a price...**

- Trees generally do not have the same level of predictive accuracy as sorne of the other regression and classification approaches.
-  Additionally, trees can be very non-robust: a small change in the data can cause a large change in the final estimated tree.

# Resources

-   [Applied Data Mining and Statistical Learning (Penn Statte-University)](https://online.stat.psu.edu/stat508/)
-   [R for statistical learning](https://daviddalpiaz.github.io/r4sl/)
-   [CART Model: Decision Tree Essentials](http://www.sthda.com/english/articles/35-statistical-machine-learning-essentials/141-cart-model-decision-tree-essentials/#example-of-data-set)
-   [An Introduction to Recursive Partitioning Using the RPART Routines](https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf)
